---
layout: post
title: B-11 Milestone 2
permalink: /page2/
---


# Feature Engineering I

In this section, we analyze the impact of distance and angle on scoring probabilities in hockey. By visualizing shot distributions and calculating goal rates, we uncover patterns that highlight how these factors influence the likelihood of scoring. These insights provide a deeper understanding of shot strategies and their effectiveness.

Q1:Using the training dataset, we engineer the following columns:
1.	**Distance from Net:**
   
  Computed as the Euclidean distance from the (x, y) coordinates of the shot to the net.
  
  Approximate the net as a single point (89, 0) or (-89, 0) and calculate the distance to the closest net.
  
2.	**Angle from Net:**
	
  The angle between the shot position and the net, calculated using the arctan2 function.
 
3.	**Is Goal:**
   
  Binary column indicating whether the shot resulted in a goal (1) or not (0).
  
4.	**Empty Net:**
   
   Binary column indicating whether the shot was on an empty net (1) or not (0).
   
   Missing values for this column (NaNs) are assumed to be 0.
  
**Steps:**

Extract only SHOT and GOAL events.
   
Add the required engineered features.
  
Clean the data by handling missing values and invalid coordinates.
	
  
 

**Visualizations:**
1.	**Histogram of Shot Counts (Goals and No-Goals) Binned by Distance:**


![image](https://github.com/user-attachments/assets/95b5f0df-4b18-4aca-8977-cfb388c06b24)

The histogram illustrates the distribution of shot counts based on the distance from the net, separated into two categories: "Goal" (red) and "No Goal" (blue). It is evident from the plot that the majority of shots occur within close proximity to the net, specifically under 20 feet. In this range, the number of goals is relatively higher compared to no-goals, reflecting the higher likelihood of scoring when a player is closer to the net. This aligns with hockey dynamics, where close-range shots provide players with better angles and reduce the goalie's reaction time.
As the distance increases, the frequency of shots decreases overall. Beyond 20 feet, the number of no-goals begins to dominate, with a sharp decline in goals. This trend continues into medium and long distances (20-60 feet), where the probability of scoring drops significantly due to reduced accuracy and the goalie's increased ability to react to the shot. At longer distances (e.g., >40 feet), no-goals make up the overwhelming majority of attempts, indicating that such shots are often speculative or intended to generate rebounds rather than direct scoring opportunities.
This pattern aligns with domain knowledge in hockey, where shorter distances offer higher scoring chances, while medium and long-range shots are more likely to be saved or miss the target entirely. Overall, the histogram accurately reflects the spatial dynamics of hockey shot outcomes and provides insights into how distance affects scoring probabilities.

2.	**Histogram of Shot Counts (Goals and No-Goals) Binned by Angle:**
![image](https://github.com/user-attachments/assets/fe20f05a-b286-4f34-8451-ce3365ef1370)

The histogram depicts the distribution of shot counts based on the angle from the net, with outcomes categorized as "Goal" (red) and "No Goal" (blue). It is clear from the plot that the majority of shots occur at extreme angles, approximately ±150 degrees, where the shot trajectory is nearly parallel to the goal line. Despite the high volume of shots at these extreme angles, the proportion of goals is significantly lower compared to no-goals, highlighting the difficulty of scoring from these positions.
In contrast, shots taken at central angles (close to 0°) are much more likely to result in goals, even though the total shot volume is relatively lower in this region. This aligns with hockey dynamics, where shots directly in front of the net provide the best scoring opportunities due to better alignment with the goal and reduced difficulty for the shooter.
The pattern of no-goals dominating across all angles, but especially at extreme angles, suggests that players often attempt shots in low-probability scenarios, possibly seeking deflections or rebounds rather than direct scoring opportunities. This histogram demonstrates that angle is a crucial determinant of scoring likelihood, with central angles offering higher probabilities of success compared to extreme angles.

3.	**2D Histogram (Distance vs Angle):**
![image](https://github.com/user-attachments/assets/4103678f-8a2c-4866-85c8-fa93ca0b9618)

The 2D histogram shows the joint distribution of hockey shots based on distance from the net and angle, with darker shades representing higher shot frequencies. Most shots cluster near the net (within 20 feet) and central angles (approximately -20° to 20°), indicating that players prioritize positions with higher scoring probabilities. A significant number of shots also occur at extreme angles (±150°) beyond 20 feet, likely as attempts for rebounds or deflections, though these are less likely to result in goals. The plot highlights a symmetric distribution across both sides of the rink and shows that shot frequency decreases significantly with increasing distance, reflecting the strategic importance of close, centrally aligned shots in hockey.

Q2: **Goal Rate by Distance and Angle**

![image](https://github.com/user-attachments/assets/406bfd08-3bed-4f8d-bba7-9bae76a57078)

![image](https://github.com/user-attachments/assets/7eeba812-f7c6-4fab-94a4-c5ebecf44f46)

The plot shows the relationship between goal rate and the angle from the net, with the goal rate peaking at central angles (close to 0°) and decreasing as the angle becomes more extreme (closer to ±75°). At 0°, where shots are directly in front of the net, the goal rate exceeds 14%, highlighting the advantage of central positioning for scoring. As the angle moves away from the centerline, the goal rate gradually declines, reflecting the difficulty of scoring from wider angles due to reduced shooting opportunities and tighter goalie coverage. This pattern emphasizes that shots taken closer to the central line are strategically more effective, whereas extreme angles are less likely to result in goals.

# Baseline Models

In our analysis, I observed that the models trained using only the distance, angle, and the combination of both features yielded exactly the same accuracy on the validation set. However, other evaluation metrics, such as precision, recall, and ROC-AUC, differed across these models. This consistency in accuracy across distinct feature sets is highly unusual and indicates a potential issue, likely a bug in the implementation or evaluation process. While I haven't identified the precise source of the bug


### Analysis and comparison of baseline models


![plot_roc](/assets/plot_roc.png)

The ROC curve shows that the **LogisticReg_Distance_Angle** model (AUC = 0.70) performs the best, followed closely by **LogisticReg_Distance_Only** (AUC = 0.69), indicating that combining features slightly improves discrimination. In contrast, **LogisticReg_Angle_Only** (AUC = 0.56) performs poorly, barely surpassing the **Random Baseline** (AUC = 0.50), highlighting that distance is a more informative feature than angle alone.


![plot_calibration_curve](/assets/plot_calibration_curve.png)

The models produce predictions concentrated at smaller mean predicted probability values because the logistic regression models are limited by their linear nature and may struggle to properly estimate probabilities for rare events (e.g., goals). In contrast, the random baseline generates probabilities uniformly across the range, without regard to the underlying data distribution. This highlights that the models are overestimating probabilities in regions with low true positive rates, indicating poor calibration and underutilization of feature complexity.


![plot_goal_rate_by_percentile](/assets/plot_goal_rate_by_percentile.png)
The plot indicates that the models effectively learn from the distance feature, as it significantly improves goal rate predictions at higher percentiles. However, the angle feature appears to be less informative, resulting in a flatter curve and limited ability to distinguish between goal probabilities.

![plot_cumulative_prop](/assets/plot_cumulative_prop.png)
Models using distance (alone or combined with angle) effectively prioritize high-probability goals in the top percentiles. The angle feature alone is insufficient to provide meaningful discrimination, highlighting the importance of distance in this classification problem.


links to [plots](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/u5tjfhpj) on wandb

link to models  : 
[LogisticReg_Distance_Angle](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/lc7a2aie)
[LogisticReg_Distance_Only](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/udgairdb)
[LogisticReg_Angle_Only](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/6cewilbn)

<!-- ----------------------------------------------------------------

We split dataset into train, test dataset:

Then start building the model:

1-	**Logistic regression:**

For Distance we got this accuracy:

Accuracy (Distance Only): **0.9029694752697427**

**Prediction:**
 ![image](https://github.com/user-attachments/assets/e99f76c4-05f3-48e4-908b-a811751f45da)

 from this figure above we can measure the performance of the desired logistic regression model, and we found that the model acts well.
 ![image](https://github.com/user-attachments/assets/002169a7-fb3c-4fb5-a056-60159fbc9296)


 Display the correlation between Goal rate and Shot probability Model Percentile and we notice that changes.


**Find cumulative proportion of goals:**
 ![image](https://github.com/user-attachments/assets/cc7bebd5-5cdb-4486-9051-b9bcbee16393)

The plot illustrates the cumulative proportion of goals as a function of the shot probability model percentile, where shots are ranked by their predicted probabilities from the logistic regression model (using distance as a feature). The cumulative proportion increases steadily, indicating that the model successfully assigns higher probabilities to shots that are more likely to result in goals


**Relibility:**
 ![image](https://github.com/user-attachments/assets/99e3a5b1-3a9e-4468-8332-e75b623f3518)

The reliability diagram for the "Distance Only" logistic regression model compares the predicted probabilities of goals (positive class) to the observed fraction of actual goals. The dotted diagonal line represents a perfectly calibrated model, where the predicted probabilities match the actual probabilities. The plot shows that the classifier's predictions are generally below the diagonal, indicating underconfidence in its goal probability predictions.
For instance, predicted probabilities in the 0.1–0.2 range correspond to actual probabilities that are slightly higher. This suggests the model slightly underestimates the likelihood of scoring at lower probability ranges. The limited spread of points near the lower probability range reflects the dataset's inherent class imbalance, as most shots result in "No Goal." This diagram highlights the need for feature enhancement (e.g., incorporating angle_from_net) or recalibration techniques to improve the alignment of predicted probabilities with observed outcomes.

**Other models:**
![image](https://github.com/user-attachments/assets/91786a56-83a7-4089-a691-92a7affbeeff)

![image](https://github.com/user-attachments/assets/fe2b8c42-fdb1-4d18-b514-5970484bacaf)

 
![image](https://github.com/user-attachments/assets/fa4c5c5c-8b12-4bc5-b436-8897bc497832)

 The ROC curve visualizes the performance of different classifiers (Distance Only, Angle Only, and Distance + Angle) in distinguishing between classes, with the True Positive Rate (TPR) plotted against the False Positive Rate (FPR). The "Distance Only" and "Angle Only" models (green and orange curves, respectively) show slightly better performance than random guessing, with AUC values of 0.51, while the "Distance + Angle" model (blue curve) aligns closely with the random baseline (red dashed line), showing no improvement (AUC = 0.50). The diagonal gray line represents a model with no predictive power (AUC = 0.5). Overall, the classifiers exhibit minimal discrimination ability, as their performance is close to random.



1.	**Distance Only:**
*	Reasonable performance, as distance is a strong predictor.
*	Fails to capture angular variations that influence scoring.
2.	**Angle Only:**
*	Poorer performance compared to distance.
*	Angle is less predictive in isolation, as scoring depends on proximity as well.
3.	**Distance and Angle:**
*	Best performance among the models.
*	The combined features complement each other, improving predictions.
4.	**Random Baseline:**
*	Lowest performance, as expected, highlighting the importance of feature-driven predictions.-->


# Feature Engineering II
In this section, we are enrich the dataset with the following new features:
* **game_seconds**: We convert the minute and second timestamp into the total amount of seconds elapsed. This value extends over the whole game, i.e. it does not reset after a period ends and instead keeps counting.
* **shot_angle**: The angle relative to the horizontal line at the shot-taker's level (in degrees).
* **last_event**: Describes what event immediately preceds the current one.
* **last_event_x_coord**: X coordinate of the last event.
* **last_event_y_coord**: Y coordinate of the last event.
* **time_from_last_event**: Amount of time elapsed from last event until the current one (in seconds).
* **distance_from_last_event**: Distance from last event.
* **rebound**: True if the last event was also a shot, otherwise False.
* **last_shot_angle**: Makes life easier for calculating the "change_in_shot_angle" feature.
* **change_in_shot_angle**: Measure of the change in shot angle between current and previous shot (in degrees). Only include if the shot is a rebound, otherwise 0.
* **speed**: Defined as the distance from the previous event, divided by the time since the previous event.
* **power_play_time**: Indicates amount of time a team of event owner has been on power-play. Resets to 0 whenever player balance is restored.
* **num_friendly**: Number of friendly non-goalie skaters on the ice for team of event owner. Never goes below 3.
* **num_opposing**: Number of opposing non-goalie skaters on the ice for team of event owner. Never goes below 3.

Link to the DataFrame for Winnipeg vs Washington game on March 12, 2018: https://wandb.ai/ds_b11/my_project/artifacts/dataset/wpg_v_wsh_2017021065/v3

# Advanced Models
## 1. Baseline XGBoost Model
### 1.1. Training/Validation Setup
- The dataset was split with validation set proportions ranging from 10% to 60%.
- The **training AUC** improves as the validation set decreases, likely due to overfitting on the smaller training set.
- The **validation AUC** peaks around a 30% validation proportion, declining as the validation set increases beyond this point.

#### Analysis
A **30% validation set** achieves the best balance, providing reliable evaluation metrics while retaining enough training data for good model performance.

#### Visualization
![Train vs Validation AUC for Different Validation Set Proportions](/assets/train_validation_auc_comparison.png)

### 1.2. Results and Comparison
- **Performance Metrics**:
  - The primary comparison metric is AUC (Area Under the Curve).
  - Logistic Regression Baseline:
    - AUC: 0.51
  - XGBoost (Distance + Angle Features):
    - AUC: 0.71

- **Comparison and Analysis**:
  - The XGBoost classifier outperformed the Logistic Regression baseline with a significantly higher AUC (0.71 vs. 0.51). This highlights XGBoost's ability to capture non-linear relationships in the data, which Logistic Regression, as a linear model, cannot do effectively.
  - Both models were trained and evaluated on the same dataset, consisting of game data from the 2016 to 2019 regular seasons. The data was split into 70% for training and 30% for validation.
  - While XGBoost demonstrates better performance, the AUC of 0.71 indicates that the predictive power of using only `distance` and `angle` features remains moderate. This suggests that adding more features could further improve model performance.

### 1.3. Curves
Below are the four key evaluation curves generated for this experiment:

1. **ROC Curve**:
   ![ROC Curve](/assets/XGB_base_ROC.png)

2. **Goal Rate vs Probability Percentile**:
   ![Goal Rate](/assets/XGB_base_Goal.png)

3. **Cumulative Proportion of Goals vs Probability Percentile**:
   ![Cumulative Proportion](/assets/XGB_base_Cum.png)

4. **Reliability Curve**:
   ![Reliability Curve](/assets/XGB_base_Rel.png)

### 1.4. Wandb Entry
For further details, metrics, and visualizations, refer to the [Wandb entry here](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/y36uczgl).


## 2. Hyperparameter Tuning for XGBoost with All Features

### 2.1. Hyperparameter Tuning Setup

To optimize the XGBoost model with all features, I performed a **randomized search** for hyperparameter tuning. This approach was chosen due to limited computational resources (CPU and memory) and the large dataset size. The randomized search efficiently explored the hyperparameter space while minimizing the computational cost.

- **Hyperparameter Distributions**:
  - `max_depth`: Randomly sampled from integers between 3 and 9.
  - `learning_rate`: Uniformly sampled between 0.01 and 0.2.
  - `n_estimators`: Randomly sampled integers between 100 and 399.
  - `subsample`: Uniformly sampled between 0.7 and 1.0.
  - `colsample_bytree`: Uniformly sampled between 0.7 and 1.0.
  
- **Evaluation Metric**: AUC was used as the evaluation metric, with 5-fold cross-validation to ensure robust performance evaluation.
  
- **Best Hyperparameters**:
  - `max_depth`: 5
  - `learning_rate`: 0.063
  - `n_estimators`: 307
  - `subsample`: 0.969
  - `colsample_bytree`: 0.757
  
- **Performance**:
  - Cross-Validation AUC: 0.7652
  - Train Set AUC: 0.7922
  - Validation Set AUC: 0.7702

---

### Figures Supporting Hyperparameter Choices

1. **Colsample Bytree vs Learning Rate**:
   - The heatmap below shows the relationship between `colsample_bytree` (fraction of features sampled for each tree) and `learning_rate` (step size).
   - A `colsample_bytree` of around 0.75 and a `learning_rate` near 0.06 achieved the highest AUC values, which aligns with the best parameters found during the search.

   ![Colsample Bytree vs Learning Rate](/assets/Tune_3.png)

2. **Number of Estimators vs Subsample**:
   - This heatmap shows how `n_estimators` (number of boosting rounds) and `subsample` (fraction of samples used per tree) impact AUC.
   - Models with a `subsample` near 0.97 and `n_estimators` between 300-350 consistently performed well, supporting the choice of `n_estimators = 307` and `subsample = 0.969`.

   ![Number of Estimators vs Subsample](/assets/Tune_2.png)

3. **Max Depth vs Learning Rate**:
   - The heatmap below visualizes the interaction between `max_depth` (tree depth) and `learning_rate`.
   - A `max_depth` of 5 paired with a smaller learning rate (around 0.06) resulted in the best AUC scores, balancing complexity and model generalization.

   ![Max Depth vs Learning Rate](/assets/Tune_1.png)

---

### Substantiating the Choice of Hyperparameters

- The **heatmaps** clearly show the performance impact of the hyperparameters. The selected values (`max_depth = 5`, `learning_rate = 0.063`, `n_estimators = 307`, `subsample = 0.969`, `colsample_bytree = 0.757`) correspond to regions with high AUC scores across all three heatmaps.
- The **interaction between parameters** (e.g., learning rate with colsample_bytree and max_depth) highlights the importance of fine-tuning combinations rather than individual parameters.
- The resulting model generalizes well with a **validation AUC of 0.77**, showing minimal overfitting compared to the training AUC of 0.79.

These findings demonstrate that the randomized search efficiently identified the hyperparameter combinations that maximized performance while adhering to computational constraints.

### 2.2. Analysis and Comparison
- **Performance**:
  - **Train Set AUC**: 0.79
  - **Validation Set AUC**: 0.77
- **Comparison with Baseline**:
  - The tuned model achieved a validation AUC of 0.77, significantly outperforming the baseline XGBoost model (AUC: 0.71) trained on only `distance` and `angle` features.
  - The improvement is attributed to two factors:
    1. **Feature Enrichment**: Using all features significantly increased the model's ability to capture data patterns.
    2. **Hyperparameter Optimization**: Randomized search enabled efficient exploration of parameter space, yielding a well-generalized model despite computational constraints.
- **Key Observations**:
  - The validation AUC (0.77) is slightly lower than the training AUC (0.7922), indicating good generalization with minimal overfitting.
  - Randomized search proved to be a practical solution for hyperparameter tuning in a resource-constrained environment, balancing performance and computational feasibility.

### Curves
Below are the four key evaluation curves generated for this experiment:

1. **ROC Curve**:
   ![ROC Curve - Best Model](/assets/XGB_tune_ROC.png)

2. **Goal Rate vs Probability Percentile**:
   ![Goal Rate](/assets/XGB_tune_Goal.png)

3. **Cumulative Proportion of Goals vs Probability Percentile**:
   ![Cumulative Proportion](/assets/XGB_tune_Cum.png)

4. **Reliability Curve**:
   ![Reliability Curve](/assets/XGB_tune_Rel.png)

### Wandb Entry
For a detailed breakdown of this experiment, hyperparameter search, and model performance, refer to the [Wandb entry here](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/00uypamk).

### Model Registry
The best-performing model has been logged to the model registry for reproducibility and future use.


## 3. Feature Selection for XGBoost Model

### 3.1. Feature Selection Strategies
To simplify the feature set and improve the interpretability of the model, the following feature selection steps were applied:

1. **Frequency Encoding for Categorical Features**:
   - Categorical variables were encoded using their frequency counts, ensuring numeric compatibility with the model.

2. **Remove Features with High Missing Values**:
   - Features with more than 50% missing values were removed. This helped ensure that the retained features had sufficient data for reliable analysis and modeling.

3. **Remove Low Variance Features**:
   - Features with more than 90% of a single value were considered low variance and removed. This step eliminated 2 features with minimal contribution to model variance, reducing noise.

4. **Select Features Based on IV (Information Value)**:
   - Computed IV values for all features and retained those with IV ≥ 0.1. This step removed 4 variables that had weak predictive power for the target variable.

5. **Remove Highly Correlated Features**:
   - Pairwise correlations were analyzed with a threshold of 0.8. One variable from each highly correlated pair was removed, leaving the most informative variable in each case. This step removed 1 redundant feature.

6. **Calculate VIF (Variance Inflation Factor)**:
   - VIF values were calculated to identify multicollinearity among features. Features with VIF > 5 were removed, provided their IV was low. No variables were removed in this step, as all retained features had low multicollinearity.

---

### 3.2. Final Feature Set
After applying the above feature selection strategies, the following 8 variables were retained:
- `y_coord`
- `distance_from_net`
- `distance_from_last_event`
- `speed`
- `time_from_last_event`
- `last_event_x_coord`
- `last_event_y_coord`
- `angle_from_net`

---

### 3.3. Model Performance with Selected Features
- **Hyperparameter Tuning**:
  - Applied hyperparameter tuning (similar to Q2) to the model trained with the selected feature set.
- **Performance Metrics**:
  - **Train Set AUC**: 0.76
  - **Validation Set AUC**: 0.74

- **Comparison with Baseline**:
  - Baseline XGBoost:
    - Validation AUC: 0.71
  - Feature-Selected XGBoost:
    - Validation AUC: 0.74
  - The feature-selected model achieved better performance while using fewer features, demonstrating the effectiveness of the selection process.

---

### 3.4. SHAP Analysis and Feature Importance
The SHAP summary plot highlights the contribution of each feature to the model's predictions. The following are key insights:

1. **Top Features**:
   - **`y_coord`** and **`distance_from_net`** are the most influential features, with the highest mean SHAP values. These features contribute significantly to the model's predictive power and align with their intuitive importance in the context of the dataset.
   - **`time_from_last_event`** is the third most important feature, indicating the temporal relationship between events is critical for accurate predictions.

2. **Moderate Importance**:
   - **`angle_from_net`**, **`speed`**, and **`last_event_y_coord`** are moderately important, suggesting spatial and velocity-based factors also play a role in model predictions.

3. **Low Importance**:
   - **`last_event_x_coord`** and **`distance_from_last_event`** have the lowest SHAP values among the selected features, but they still provide unique information that improves the model's overall performance.

4. **Advantages of SHAP**:
   - SHAP enables a transparent interpretation of the model by quantifying each feature's contribution to individual predictions and the overall model.
   - The analysis validates the feature selection process, as all retained features contribute meaningfully to the model.

---
The bar plot below ranks the top 8 features by their mean SHAP values:

![Top Feature Importances based on SHAP](/assets/importance.png)

---
### 3.5. Advantages of the Selected Approach
1. **Reduced Dimensionality**:
   - The feature set was reduced from the original size to only 8 features, leading to a simpler and more interpretable model.

2. **Elimination of Redundancy**:
   - Highly correlated features and those with high multicollinearity (VIF > 5) were systematically removed, ensuring that the retained features provided independent information to the model.

3. **Improved Predictive Power**:
   - By focusing on features with IV ≥ 0.1, the selection process retained variables with a strong predictive relationship to the target, improving model efficiency.

4. **Handling Data Quality Issues**:
   - Features with high missing values and low variance were removed early, reducing noise and ensuring that the retained features had sufficient variation for effective modeling.

5. **Efficient Use of Computational Resources**:
   - The reduced feature set decreased the computational complexity of the model, leading to faster training times and reduced memory usage without sacrificing performance.

### 3.6. Curves
Below are the updated figures corresponding to the best model with the selected feature set:

1. **ROC Curve**:
   ![ROC Curve - Feature Selected Model](/assets/XGB_fea_ROC.png)

2. **Goal Rate vs Probability Percentile**:
   ![Goal Rate - Feature Selected Model](/assets/XGB_fea_Goal.png)

3. **Cumulative Proportion of Goals vs Probability Percentile**:
   ![Cumulative Proportion - Feature Selected Model](/assets/XGB_fea_Cum.png)

4. **Reliability Curve**:
   ![Reliability Curve - Feature Selected Model](/assets/XGB_fea_Rel.png)

---

### 3.7. Wandb Entry and Model Registry
- For detailed logs, metrics, and insights, refer to the [Wandb entry here](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/ma6528kk).
- The feature-selected model has been logged to the model registry for future use and reproducibility.

# Give it your best shot!
## Four Techniques and Methods Tried
## 1. New Novel Metrics and Analysis
1. **ROC AUC (Receiver Operating Characteristic Area Under the Curve)**:
   - **Purpose**: Measures the model's ability to distinguish between positive and negative classes.
   - **Significance**: AUC close to 1 indicates excellent discrimination, while a value near 0.5 indicates random guessing. It is particularly useful in imbalanced datasets.

2. **Accuracy**:
   - **Purpose**: Measures the overall proportion of correctly predicted instances.
   - **Significance**: Simple and intuitive but can be misleading in imbalanced datasets. For example, predicting the majority class can result in high accuracy despite poor performance.

3. **Precision**:
   - **Purpose**: Evaluates the proportion of true positives among all predicted positives.
   - **Significance**: Important in scenarios where false positives are costly, such as medical diagnoses.

4. **Recall**:
   - **Purpose**: Measures the proportion of actual positive samples correctly identified by the model.
   - **Significance**: Useful in cases where false negatives are costly, such as in cancer detection or fraud prevention.

5. **F1 Score**:
   - **Purpose**: The harmonic mean of Precision and Recall, balancing the two metrics.
   - **Significance**: Suitable for imbalanced datasets, as it considers both false positives and false negatives.

6. **Brier Score**:
   - **Purpose**: Evaluates the calibration of predicted probabilities. Lower scores indicate better-calibrated predictions.
   - **Significance**: Useful for tasks requiring reliable probability predictions, such as insurance or risk modeling.

7. **Matthews Correlation Coefficient (MCC)**:
   - **Purpose**: Measures the quality of binary classifications by considering all four confusion matrix elements (TP, FP, TN, FN).
   - **Significance**: A balanced metric particularly useful for imbalanced datasets. Values range from -1 (perfectly wrong) to 1 (perfectly correct), with 0 indicating random predictions.

8. **Average Precision Score**:
   - **Purpose**: The area under the Precision-Recall curve, reflecting the model's ability to rank positive samples higher.
   - **Significance**: More meaningful than accuracy in imbalanced datasets, as it focuses on the quality of positive predictions.

---

### Summary
These metrics provide a comprehensive evaluation of the model's performance across different dimensions:
- **Discrimination**: ROC AUC measures the model's ability to distinguish between classes.
- **Calibration**: Brier Score evaluates the accuracy of probability estimates.
- **Imbalanced Data Handling**: Precision, Recall, F1 Score, MCC, and Average Precision Score are crucial for assessing models on imbalanced datasets.
- **Overall Accuracy**: Accuracy gives a general view but should be considered alongside other metrics to avoid misleading conclusions.

Using a combination of these metrics ensures a well-rounded assessment, identifying strengths and weaknesses in the model's predictions from various perspectives.

## 2. Different Model Types Comparison
### Metrics Overview
The following table summarizes the performance metrics of three models (Random Forest, XGBoost, and LightGBM) trained with default parameters:

| Metric                             | Random Forest       | XGBoost            | LightGBM          |
|------------------------------------|---------------------|--------------------|--------------------|
| **ROC AUC**                        | 0.7194              | **0.7586**         | 0.7518             |
| **Accuracy**                       | 0.9119              | **0.9125**         | 0.9122             |
| **Precision**                      | 0.6996              | **0.8085**         | 0.7201             |
| **Recall**                         | 0.0663              | 0.0582             | **0.0675**         |
| **F1 Score**                       | 0.1212              | 0.1086             | **0.1234**         |
| **Brier Score**                    | 0.0773              | **0.0741**         | 0.0747             |
| **Matthews Correlation Coefficient (MCC)** | 0.1973       | 0.2025             | **0.2027**         |
| **Average Precision Score**        | 0.2575              | **0.2966**         | 0.2873             |

---
### Observations
- **Best Overall Model**: XGBoost demonstrated the best overall performance across multiple metrics, excelling in ROC AUC, precision, Brier Score, and Average Precision Score.
- **Strengths of LightGBM**:
  - LightGBM outperformed in recall and F1 Score, making it better suited for applications requiring a balance between precision and recall.
- **Random Forest Limitations**:
  - Random Forest lagged behind XGBoost and LightGBM in most metrics, indicating that it may not be the best choice for this task with default parameters.

---

### Recommendations
- **Focus on XGBoost**: Given its strong performance in ROC AUC, precision, and probability calibration, XGBoost is the leading candidate for further tuning.
- **Explore LightGBM for Recall Optimization**: LightGBM's superior recall and F1 Score suggest it could be useful in scenarios where identifying true positives is critical.
- **Further Improvements**: Adjust hyperparameters and explore regularization techniques to enhance recall and F1 Score for all models.

### Wandb Experiment Logs
Below are links to the detailed experiment logs for each model:[Different Models](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/btj67fa7)

## 3. More advanced feature selection
### Advanced Feature Selection: RFE, Mutual Information, and PCA

### Introduction to Methods
1. **RFE (Recursive Feature Elimination)**:
   - Iteratively removes the least important features based on a model's feature importance scores.
   - Retains the top-ranked features that contribute the most to the model's predictions.
   - Goal: Select features that are most relevant to the target variable while reducing redundancy.

2. **Mutual Information**:
   - Measures the mutual dependence between features and the target variable.
   - Retains features with the highest mutual information scores, indicating strong relevance to the target.
   - Goal: Identify features that maximize shared information with the target variable.

3. **PCA (Principal Component Analysis)**:
   - A dimensionality reduction technique that transforms features into a set of uncorrelated principal components.
   - Retains the top components that explain the most variance in the data.
   - Goal: Reduce feature space while retaining most of the variability in the dataset.

---

### Results and Analysis
The following table summarizes the performance metrics of XGBoost models trained with all features and the selected top 10 features from RFE, Mutual Information, and PCA:

| Metric                             | All Fea(Drop 2 Cat) | RFE                | Mutual Information | PCA                |
|------------------------------------|---------------------|--------------------|--------------------|--------------------|
| **ROC AUC**                        | 0.7392              | 0.7389             | 0.7291             | 0.7352             |
| **Accuracy**                       | 0.9123              | 0.9123             | 0.9121             | 0.9122             |
| **Precision**                      | 0.5055              | **0.5080**         | 0.4792             | 0.4958             |
| **Recall**                         | **0.0217**          | 0.0151             | 0.0136             | 0.0207             |
| **F1 Score**                       | **0.0416**          | 0.0292             | 0.0265             | 0.0398             |
| **Brier Score**                    | 0.0747              | 0.0749             | 0.0754             | **0.0751**         |
| **Matthews Correlation Coefficient (MCC)** | **0.0908** | 0.0758             | 0.0692             | 0.0875             |
| **Average Precision Score**        | **0.2227**          | 0.2146             | 0.2071             | 0.2149             |

---

### Key Observations
1. **Overall Performance**:
   - The model trained with **all features** (Drop 2 Cat Features)slightly outperformed feature selection methods in most metrics, particularly in ROC AUC (0.7392) and MCC (0.0908).
   - Among feature selection methods, **PCA** and **RFE** delivered relatively similar performance, while **Mutual Information** lagged behind in all key metrics.

2. **Precision vs Recall**:
   - While RFE achieved the highest precision (0.5080), its recall (0.0151) was the lowest, resulting in a suboptimal F1 Score (0.0292).
   - Mutual Information had the lowest recall (0.0136) and precision (0.4792), indicating that it struggled to identify positive samples effectively.

3. **F1 Score**:
   - The **all features** model achieved the best F1 Score (0.0416), balancing precision and recall more effectively than feature selection methods.
   - PCA followed closely (0.0398), indicating it retained most of the important information in the data.

4. **Brier Score and Calibration**:
   - The Brier Score differences were minor, with PCA achieving the best score (0.0751), suggesting slightly better-calibrated probability predictions compared to other methods.

5. **Average Precision Score**:
   - The **all features** model achieved the highest Average Precision Score (0.2227), indicating better ranking of positive samples.
   - PCA and RFE had comparable performance, while Mutual Information scored the lowest.

---

### Conclusion
1. **Best Overall Model**:
   - The XGBoost model trained with **all features** (Drop 2 Cat Features)achieved the best overall performance, particularly in ROC AUC, F1 Score, MCC, and Average Precision Score.

2. **Feature Selection Observations**:
   - **RFE** provided moderate performance, retaining the most important features based on model-based importance but struggled with recall.
   - **Mutual Information** underperformed, likely due to its reliance on univariate relationships, which may not capture interactions among features.
   - **PCA** performed well, closely matching the all-features model in most metrics, making it a viable option for dimensionality reduction.

3. **Recommendation**:
   - If computational resources are a concern or interpretability is required, **PCA** offers a good trade-off between performance and feature reduction.
   - For tasks requiring a focus on precision, **RFE** may be preferred. However, for the best performance across the board, retaining all features remains the most effective approach.

### Wandb Experiment Logs
Below are links to the detailed experiment logs for each model:[Advanced Feature Selection](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/un6xi07q)

## 4. Regularization
### Regularization Experiment: Testing Different Regularization Levels

### Regularization Parameters
The following table outlines the regularization parameter combinations tested in this experiment:

| Regularization Level       | `reg_alpha` | `reg_lambda` | `gamma` |
|----------------------------|-------------|--------------|---------|
| **Light Regularization**   | 0.01        | 1            | 0       |
| **Moderate Regularization**| 0.1         | 1            | 0.1     |
| **High Regularization**    | 1           | 10           | 1       |
| **Comprehensive Regularization** | 0.1    | 5            | 0.5     |
| **Extreme Regularization** | 10          | 20           | 5       |

---

### Results and Analysis
The following table summarizes the performance of XGBoost with these regularization levels:

| Metric                             | Light Regularization | Moderate Regularization | High Regularization | Comprehensive Regularization | Extreme Regularization |
|------------------------------------|----------------------|-------------------------|---------------------|-----------------------------|------------------------|
| **ROC AUC**                        | 0.7480               | 0.7481                  | **0.7484**          | 0.7483                      | 0.7455                |
| **Accuracy**                       | **0.9127**           | 0.9126                  | 0.9126              | 0.9126                      | 0.9124                |
| **Precision**                      | 0.6019               | 0.5882                  | **0.6066**          | 0.5918                      | 0.7021                |
| **Recall**                         | **0.0151**           | 0.0154                  | 0.0132              | 0.0138                      | 0.0039                |
| **F1 Score**                       | **0.0294**           | 0.0300                  | 0.0258              | 0.0269                      | 0.0078                |
| **Brier Score**                    | **0.0740**           | **0.0740**              | 0.0740              | **0.0740**                  | 0.0743                |
| **Matthews Correlation Coefficient (MCC)** | **0.0852**      | 0.0849                  | 0.0801              | 0.0805                      | 0.0480                |
| **Average Precision Score**        | **0.2329**           | 0.2334                  | 0.2330              | **0.2334**                  | 0.2282                |

---

### Observations
1. **Overall Performance**:
   - **Light Regularization** achieves the best overall balance across most metrics. It has the highest recall (0.0151), F1 Score (0.0294), and MCC (0.0852), making it the optimal choice for this task.
   - While High Regularization has the best ROC AUC (0.7484) and precision (0.6066), it sacrifices recall and F1 Score, which are critical for imbalanced datasets.

2. **Overfitting and Calibration**:
   - **Light Regularization** achieves the lowest Brier Score (0.0740), indicating well-calibrated probability predictions. This reflects its ability to prevent overfitting without sacrificing generalization.

3. **Extreme Regularization**:
   - As expected, Extreme Regularization severely underfits, leading to the lowest recall (0.0039) and F1 Score (0.0078), demonstrating the dangers of overly strict constraints.

4. **Comprehensive and Moderate Regularization**:
   - Both Comprehensive and Moderate Regularization perform similarly, but they do not offer any clear advantages over Light Regularization in key metrics like F1 Score and MCC.

---

### Conclusion and Recommendation
The **Light Regularization** configuration (`reg_alpha = 0.01, reg_lambda = 1, gamma = 0`) emerges as the best option for this task:
- It achieves the best balance between precision and recall, leading to the highest F1 Score and MCC.
- It avoids overfitting while maintaining high probability calibration (lowest Brier Score).

### Wandb Experiment Logs
Below are links to the detailed experiment logs for each model:[Regularization](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/55c9klyj)

## 5. The Best Model
### Model Setup Optimal Configuration
After evaluating various configurations, the optimal model was determined using:
- **Model**: XGBoost
- **Feature Set**: Selected 13 features
- **Regularization Parameters**: 
  - `reg_alpha = 0.01`
  - `reg_lambda = 1`
  - `gamma = 0`
- **Other Parameters**: 
  - Randomly searched and optimized previously for the dataset.
- **Evaluation**:
  - **Train Set AUC**: 0.77
  - **Validation Set AUC**: 0.75

---

### Final Model Performance
Below are the key evaluation figures for the best model:

1. **ROC Curve**:
   ![ROC Curve](/assets/Best_ROC.png)

   - The ROC curve demonstrates the model's ability to distinguish between positive and negative classes, with a validation AUC of 0.75.

2. **Goal Rate vs Probability Percentile**:
   ![Goal Rate](/assets/Best_Goal.png)

   - The goal rate analysis shows how well the model ranks probability predictions against actual outcomes.

3. **Cumulative Proportion of Goals vs Probability Percentile**:
   ![Cumulative Proportion](/assets/Best_Cum.png)

   - The cumulative proportion curve evaluates the model's accuracy in predicting goal events.

4. **Reliability Curve**:
   ![Reliability Curve](/assets/Best_Rel.png)

   - The reliability curve highlights the calibration of the model's predicted probabilities, showing good alignment between predicted and observed probabilities.

---

### Wandb Experiment Logs
For detailed logs, metrics, and visualizations, refer to the following Wandb entries:[The Best Model Experiment](https://wandb.ai/ds_b11/IFT6758.2024-B11/runs/0w9d22mj)

---

# Evaluate on test set
### 1. **test models on Regular**:

The **ROC plot** below show that :
- The inclusion of both distance and angle features improves performance slightly over distance alone, as seen previously in part 3.
- The poor performance of LogisticReg_Angle_Only suggests that the angle feature is not independently predictive and needs to be combined with other features for better results.
- XGBoost outperforms the logistic regression models, likely due to its ability to capture non-linear relationships and interactions between features. However, it would be expected to demonstrate even greater performance compared to logistic regression, given its advanced modeling capabilities.

![plot_calibration_curve_test_reg](/assets/plot_roc_test_reg_corrected.png)


Analysis the results on the plots representing **goal rate** and **cumulative porportoions of goals**: 
- Best_Model and XGBoost consistently outperform other models in both goal rate and cumulative goal proportion, demonstrating their ability to utilize complex feature interactions.
- Distance is the most impactful feature, as seen by the strong performance of LogisticReg_Distance_Only, while angle alone offers limited predictive value.
- Feature Interaction and Non-linearity: XGBoost and Best_Model highlight the importance of non-linear models in this task, as they outperform logistic regression in prioritizing high-probability goals.

![plot_calibration_curve_test_reg](/assets/plot_goal_rate_by_percentile_test_reg_corrected.png)
![plot_calibration_curve_test_reg](/assets/plot_cumulative_prop_test_reg_corrected.png)


The **calibration curve** reveals that XGBoost demonstrates better calibration compared to the logistic regression models, as its curve aligns more closely with the perfect calibration line (dotted diagonal). This indicates that XGBoost's predicted probabilities are more representative of the actual fraction of positives across the probability range.

![plot_calibration_curve_test_reg](/assets/plot_calibration_curve_test_reg_corrected.png)



### 2. **test models on Playoffs**:

This **calibration curve** for the TestSet Playoffs 2020/21 has oome significant instability across the models when predicting probabilities. That must be due to the lack of data in testset. Models trained on regular-season data may struggle to generalize to playoffs, leading to inconsistent and unstable probability estimates. The extreme peaks and drops in models like Best_Model suggest overfitting to specific data points, highlighting a lack of robustness.
![plot_calibration_curve_test_reg](/assets/plot_calibration_curve_test_playoffs_corrected.png)

The three plots for the **playoffs test set** demonstrate stable results that are consistent with those observed in the **regular season test set**, despite the playoff dataset being significantly smaller (10x smaller). The ROC curve shows that **XGBoost** and **Best_Model** continue to outperform the logistic regression models, achieving the highest AUC scores (0.72), while **LogisticReg_Distance_Angle** and **LogisticReg_Distance_Only** follow closely. The goal rate and cumulative goal proportion plots highlight that distance remains the most influential feature, as models relying on it achieve better calibration and predictive power compared to **LogisticReg_Angle_Only**, which underperforms consistently. 

This stability across both datasets suggests that the probability of scoring a goal does not vary significantly between the regular season and playoffs. Players appear to maintain similar strategies and shot tactics across different parts of the season, indicating that the game dynamics influencing goal expectancy remain constant.

![plot_calibration_curve_test_reg](/assets/plot_roc_test_playoffs_correted.png)
![plot_calibration_curve_test_reg](/assets/plot_goal_rate_by_percentile_test_playoffs_corrected.png)
![plot_calibration_curve_test_reg](/assets/plot_cumulative_prop_test_playoffs_corrected.png)



